{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CommonCrawl Python Code For Data Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "try:\n",
    "    from cStringIO import StringIO\n",
    "except:\n",
    "    from StringIO import StringIO\n",
    "final_list = []\n",
    "l=['filter==status:200','matchType=domain',\"www.cnn.com\"]\n",
    "f1=open('commoncrawl_guns.txt','w')\n",
    "f2=open('commoncrawl_guns.txt','w')\n",
    "response = requests.get('https://index.commoncrawl.org/CC-MAIN-2019-13-index?'+'url='+l[2]+'&'+l[1]+'&'+l[0]+'&'+'output=json')\n",
    "pages = [json.loads(x) for x in response.content.strip().split('\\n')]\n",
    "print len(pages)\n",
    "a=len(pages)\n",
    "for i in range(0,a):\n",
    "    page = pages[i]\n",
    "    o, l = int(page['offset']), int(page['length'])\n",
    "    o_end = o + l - 1\n",
    "    f2.write(str(page)) \n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "    resp = requests.get(prefix + page['filename'], headers={'Range': 'bytes={}-{}'.format(o, o_end)})\n",
    "    raw_data = StringIO(resp.content)\n",
    "    f = gzip.GzipFile(fileobj=raw_data)\n",
    "    data = f.read()\n",
    "    try:\n",
    "        warc, header, response = data.strip().split('\\r\\n\\r\\n',2)    \n",
    "        soup = BeautifulSoup(response,from_encoding=\"windows-1259\")\n",
    "        if soup.body:\n",
    "            text = soup.findAll('p')\n",
    "            finaltext = \"\"\n",
    "            for tag in text:\n",
    "                print tag.getText()\n",
    "                if  \"gun\" in tag.getText():\n",
    "                    print (\"sampreeth\")\n",
    "                    finaltext = finaltext + tag.getText().encode(\"UTF-8\")      \n",
    "            final_list.append(finaltext)\n",
    "            f1.write(finaltext)\n",
    "            print (finaltext)\n",
    "    except BaseException:\n",
    "        print \"exception caught\"\n",
    "        continue\n",
    "f1.close()\n",
    "f2.close()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter R Code For Data EXtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(twitteR)\n",
    "\n",
    "api_key <- \"dl5ZSObFerh86fRmmdOzRM66r\"\n",
    "api_secret <- \"73DkusXaGC6pLma12YeqrMFfvPxCKQ9oXyy72HwGffMmcNPn0x\"\n",
    "access_token <- \"1038150894649585664-kaVPxjxThMhBwDO3RZaHhz8hWiXZWT\"\n",
    "access_token_secret <- \"lhLqmdmXjkhAYcKW08q1HdsGSv4cXGmymdx7eGFXnVxvM\"\n",
    "setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n",
    "\n",
    "\n",
    "\n",
    "tweets <- searchTwitter('%shooting% -filter:retweets', n=100000, lang=\"en\")\n",
    "\n",
    "if(length(tweets)>0)\n",
    "{\n",
    "  tweets.df <- twListToDF(tweets)\n",
    "  tweets.df <- data.frame(tweets.df, stringsAsFactors = FALSE)\n",
    "  write.csv(tweets.df, \"/home/sampreeth/Documents/twitter_data_18_04shoooting.csv\", row.names = F )\n",
    "  twitter_data <- read.csv(\"/home/sampreeth/Documents/twitter_data_18_04shooting.csv\", stringsAsFactors = FALSE)\n",
    "  tweets_text_data <- gsub(\"\\n\", \" \", twitter_data$text)\n",
    "  tweets_text_data.char <- as.character(tweets_text_data)\n",
    "  Encoding(tweets_text_data.char) <- \"UTF-8\"\n",
    "  write.table(tweets_text_data.char, file = \"/home/sampreeth/Documents/twitter_data_18_04shoooting.txt\", row.names = F, col.names = F)    \n",
    "} \n",
    "else \n",
    "{\n",
    "  print (\"No tweets\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York Times Python Data Extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nytimesarticle import articleAPI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "api = articleAPI(\"BUzfjKlJ4dGGeoTSUAKVP5JgHTwgPeJZ\")\n",
    "links=[]\n",
    "f=open('nytarticlesday_ISIS_19.txt','w')\n",
    "f1=open('nytarticlesday_ISIS_links.txt','w')\n",
    "try:\n",
    "    for a in range(0,50):\n",
    "        articles = api.search(q=\"isis\", begin_date=20190101,end_date=20190418,page=a)\n",
    "        for i in range(0,len(articles['response']['docs'])):\n",
    "            url = articles['response']['docs'][i]['web_url']\n",
    "            data = requests.get(url)\n",
    "            f1.write(url)\n",
    "            f1.write(\"\\n\")\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            soup.prettify()\n",
    "            for j in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[j].get_text())\n",
    "            links.append(url)\n",
    "    f.close()\n",
    "    print(len(links))\n",
    "except:\n",
    "    print(\"We got only\",len(links),\" articles for selected period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code to remove Duplicates and Give top 10 from word count and word-Co-accurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data1 = pd.read_csv(\"part-allcoco\", sep=\"\\t\", header=None)\n",
    "data1.columns = ['word', 'count']\n",
    "df_sort=data1.sort_values('count',ascending=False)\n",
    "top10=df_sort.head(50)\n",
    "data = pd.DataFrame(top10.values, columns=['word', 'count'])\n",
    "droplist=[]\n",
    "i=0\n",
    "while(i<len(data)):\n",
    "    j=i\n",
    "    while(j<len(data)):\n",
    "        a=data['word'][i]\n",
    "        b=data['word'][j]\n",
    "        a1=a.split(\",\")\n",
    "        b1=b.split(\",\")\n",
    "        #print(a1,b1)\n",
    "\n",
    "        if (a1[0]==b1[1] and b1[0]==a1[1]):\n",
    "            print(\"yes\")\n",
    "            data['count'][i]=data['count'][i]+data['count'][j]\n",
    "            droplist.append(j)\n",
    "        j=j+1\n",
    "    i=i+1    \n",
    "\n",
    "      \n",
    "data2=data.drop(data.index[droplist])\n",
    "df_sort1=data2.sort_values('count',ascending=False)\n",
    "top10r=d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10r\n",
    "top10r.to_csv('out.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
